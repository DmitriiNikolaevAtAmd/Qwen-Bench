work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:llama3.1_8B-pretrain}
workspace: ${PRIMUS_WORKSPACE:./output}

# TPrimat/Megatron defaults (env overrides applied at runtime via utils/configure.py)
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
sequence_parallel: false
global_batch_size: 128
micro_batch_size: 2
seq_length: 8192
encoder_seq_length: 8192
gradient_accumulation_steps: 1
use_distributed_optimizer: true
accumulate_allreduce_grads_in_fp32: true
use_flash_attn: true
fp32_residual_connection: false
train_iters: 50
lr_decay_iters: 50
lr_warmup_iters: 2
seed: 42
init_method_std: 0.02
adam_beta1: 0.9
adam_beta2: 0.95
tokenizer_type: HuggingFaceTokenizer
split: "100,0,0"
eval_iters: 0
eval_interval: 999999
log_interval: 1
log_memory_to_tensorboard: true
log_throughput: true
disable_tensorboard: true
disable_wandb: true
disable_mlflow: true
log_timers_to_tensorboard: false
log_learning_rate_to_tensorboard: false
log_loss_scale_to_tensorboard: false
profile: true
use_pytorch_profiler: true
torch_profiler_with_stack: false
torch_profiler_record_shapes: false
torch_profiler_use_gzip: false
recompute_granularity: selective
recompute_method: null
recompute_num_layers: null

modules:
  pre_trainer:
    framework: megatron
    config: pre_trainer.yaml

    # model to run
    model: llama3.1_8B.yaml
    overrides:
      # log
      wandb_project: "Primus_DeepSeek_Pretrain"
      # disable_wandb: false
      # disable_tensorboard: false
      stderr_sink_level: DEBUG

      log_avg_skip_iterations: 2
      log_avg_reset_interval: 50

      train_iters: 50
      micro_batch_size: 2
      global_batch_size: 128

      seq_length: 8192
      max_position_embeddings: 8192

      lr: 1.0e-5
      min_lr: 0.0
      lr_warmup_iters: 2
      lr_decay_iters: null
      lr_decay_style: cosine
      weight_decay: 0.1
      adam_beta1: 0.9
      adam_beta2: 0.95
      eod_mask_loss: true
      init_method_std: 0.008
      norm_epsilon: 1.0e-6

      # parallel
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      overlap_grad_reduce: true
      overlap_param_gather: true
      gradient_accumulation_fusion: false

      # data â€” mock_data is overridden to false by configure.py; real data comes
      # from --data_path + --split on the CLI (do NOT set train_data_path here,
      # it creates blend_per_split which conflicts with --split)
      mock_data: true

      # ckpt
      finetune: false
      auto_continue_train: false
      load: null
      no_load_optim: null
      no_load_rng: null
      save: null
      save_interval: 20000
      no_save_optim: null
      no_save_rng: null
      disable_last_saving: true
      ckpt_format: torch

      # Turbo
      enable_primus_turbo: true
      use_turbo_attention: false
      use_turbo_grouped_mlp: false

      # profiling (Primus namespace: profile, use_pytorch_profiler, profile_step_start/end only)
      profile: true
      use_pytorch_profiler: true

      # Cross entropy flags
      # cross_entropy_fusion_impl: "te"
      # cross_entropy_loss_fusion: true
