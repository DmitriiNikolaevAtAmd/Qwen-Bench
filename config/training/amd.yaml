# 1. Training Algorithm (apple-to-apple)
micro_batch_size: 1
seq_length: 2048
gradient_accumulation: 8
global_batch_size: 64
learning_rate: 3.0e-4
lr_scheduler: cosine
min_lr: 0.0
warmup_steps: 10
train_iters: 50
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
init_method_std: 0.02
checkpointing: false
data_format: megatron
dataset: bc
data_path: null
data_split: "100,0,0"
num_samples: 50000
num_workers: 4

# 2. Parallelism (apple-to-apple)
parallel:
  tensor: 1
  pipeline: 1
  data: 8
  context: 1
  sequence: false
  virtual_pipeline: null

# 3. Precision (apple-to-apple)
precision: bf16
fp8_hybrid: false
fp8_param: false
fp32_residual_connection: false
distributed_optimizer: true
grad_reduce_in_fp32: false
overlap_grad_reduce: false
overlap_param_gather: false

# 4. Fusions & Attention (best-of-breed)
fusions:
  bias_activation: true
  bias_dropout: true
  masked_softmax: true
  persist_layer_norm: false
  apply_rope: true
  cross_entropy_loss: true
  gradient_accumulation: false
  flash_attn: true
  nvte_fused_attn: false
  nvte_flash_attn: false
  primus_turbo: true
  turbo_attention: true

# Recompute (full recompute needed on AMD without flash attention)
recompute:
  granularity: full
  method: uniform
  num_layers: 1

# Profiling
profiling:
  enabled: false
  step_start: 1
  step_end: 5

# Data verification
verify_data: false
verify_samples: 100
verify_full_scan: false
