name: llama
display_name: Llama 3.1 8B
model_name_or_path: meta-llama/Llama-3.1-8B
tokenizer_path: meta-llama/Llama-3.1-8B

architecture:
  num_layers: 32
  hidden_size: 4096
  num_attention_heads: 32
  num_query_groups: 8
  ffn_hidden_size: 14336
  max_position_embeddings: 131072
  normalization: RMSNorm
  norm_epsilon: 1.0e-5
  swiglu: true
  rotary: true
  untie_embeddings_and_output_weights: false
